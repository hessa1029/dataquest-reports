---
title: "Building a Spam Filter with Naive Bayes"
output: html_document
---

In this project, we will be tackling the challenge of identiyfing potential spam messages to allow for filtering out from the important SMS messages that we receive. In order to do this, we'll be using the Naive Bayes algorithm and a previously classified dataset of SMS messages. We will hopefully be able to use the previous messages to allow our algorithm to correctly identify spam messages with high accuracy.

This dataset comes from the UCI Machine Learning Repository and was generated by Tiago A. Almeida and Jose Maria Gomez Hidalgo.

```{r}
library(readr)
library(tidyverse)
set.seed(1)

spam = read.csv('SMSSpamCollection', sep='\t', header = F)
colnames(spam) <- c('label', 'sms')
head(spam)
```
```{r}
print(dim(spam))
```

```{r}
spam %>%
  group_by(label) %>%
  summarize(percentage = n() * 100 / nrow(spam)) %>%
  arrange(desc(percentage))
```

Out goal in the project is to maximize our predictive ability of our algorithm. See we intend to use this to classify messages that we have not seen before, we will employ the train_cv_test split to divide our data between a training set, the cross-validation set and a final test set to represent our unseen messages. We will do an 80/10/10 split of the data.

```{r}
n = nrow(spam)
n.training = 2547
n.cv = 318
n.test = 319

train.indices = sample(1:n, size = n.training, replace = FALSE)
remaining.indices = setdiff(1:n, train.indices)

cv.indices = remaining.indices[1:318]
test.indices = remaining.indices[319:length(remaining.indices)]

spam.train = spam[train.indices,]
spam.cv = spam[cv.indices,]
spam.test = spam[test.indices,]

print(mean(spam.train$label == 'ham'))
print(mean(spam.cv$label == 'ham'))
print(mean(spam.test$label == 'ham'))
```

The percentage of "ham" messages in each subset is relatively close to the original 87%, so we should be okay for analysis.

We are going to work with our training set to train the model. In order to do this, we need to split out words and clean up formatting from the SMS column so that we can use each word in our vocabulary for our Naive Bayes algorithm.

```{r}
tidy.train = spam.train %>%
  mutate(
    sms = tolower(sms),
    sms = str_replace_all(sms, '[[:punct:]]', ''),
    sms = str_replace_all(sms, '[[:digit:]]', ' '),
    sms = str_replace_all(sms, '[\u0094\u0092\n\t]', ' ')
  )
```

Now we'll create our vocabulary of words from the training set.

```{r}
vocabulary = NULL

messages = pull(tidy.train, sms)
for (m in messages) {
  words = str_split(m, ' ')[[1]]
  words = words[!words %in% '']
  vocabulary = c(vocabulary, words)
}

vocabulary = unique(vocabulary)
```


```{r}
p.spam = mean(tidy.train$label == 'spam')
p.ham = mean(tidy.train$label == 'ham')

p.spam
p.ham
```

```{r}
spam.messages = tidy.train %>%
  filter(label == 'spam') %>%
  pull('sms')

ham.messages = tidy.train %>%
  filter(label == 'ham') %>%
  pull('sms')

spam.words = NULL
for (m in spam.messages) {
  words = str_split(m, ' ')[[1]]
  spam.words = c(spam.words, words)
}

ham.words = NULL
for (m in ham.messages) {
  words = str_split(m, ' ')[[1]]
  ham.words = c(ham.words, words)
}

n.spam = length(unique(spam.words))
n.ham = length(unique(ham.words))
n.vocabulary = length(vocabulary)
alpha = 1
```

```{r}
spam.counts = list()
ham.counts = list()
spam.probs = list()
ham.probs = list()

for (v in vocabulary) {
  spam.counts[[v]] = 0
  ham.counts[[v]] = 0
  
  for (m in spam.messages) {
    words = str_split(m, ' ')[[1]]
    spam.counts[[v]] = spam.counts[[v]] + sum(words == v)
  }
  
  for (m in ham.messages) {
    words = str_split(m, ' ')[[1]]
    ham.counts[[v]] = ham.counts[[v]] + sum(words == v)
  }
  
  spam.probs[[v]] = (spam.counts[[v]] + alpha) / (n.spam + alpha * n.vocabulary)
  ham.probs[[v]] = (ham.counts[[v]] + alpha) / (n.ham + alpha * n.vocabulary)
}
```

```{r}
classify <- function(message) {
  p.spam.given.message = p.spam
  p.ham.given.message = p.ham
  
  clean.message <- tolower(message)
  clean.message <- str_replace_all(clean.message, '[[:punct:]]', '')
  clean.message <- str_replace_all(clean.message, '[[:digit:]]', ' ')
  clean.message <- str_replace_all(clean.message, '[\u0094\u0092\n\t]', ' ')
  words = str_split(clean.message, ' ')[[1]]
  
  for (w in words) {
    wi.spam.prob = ifelse(w %in% vocabulary,
                          spam.probs[[w]],
                          1)
    wi.ham.prob = ifelse(w %in% vocabulary,
                         ham.probs[[w]],
                         1)
    
    p.spam.given.message = p.spam.given.message * wi.spam.prob
    p.ham.given.message = p.ham.given.message * wi.ham.prob
  }
  
  result = case_when(
  p.spam.given.message >= p.ham.given.message ~ 'spam',
  p.spam.given.message < p.ham.given.message ~ 'ham'
  )
  
  return(result)
}



final.train <- tidy.train %>%
  mutate(
    prediction = unlist(map(sms, classify))
  ) %>%
  select(label, prediction, sms)

confusion = table(final.train$label, final.train$prediction)
accuracy = (confusion[1,1] + confusion[2,2]) / nrow(final.train)
```

```{r}
confusion
accuracy
```

Our model seems to perform with 88% accuracy on training set, but we'll want to see performance on the test set eventually for true performance.

# Hyperparameter Tuning

```{r}
alpha.grid = seq(0.1, 1, by = 0.1)
cv.accuracy = NULL
for (a in alpha.grid) {
  spam.probs = list()
  ham.probs = list()
  
  for (v in vocabulary) {
    
    spam.probs[[v]] = (spam.counts[[v]] + a) / (n.spam + a * n.vocabulary)
    ham.probs[[v]] = (ham.counts[[v]] + a) / (n.ham + a * n.vocabulary)
  }
  
  cv = spam.cv %>%
    mutate(prediction = unlist(map(sms, classify))) %>%
    select(label, prediction, sms)
  
  confusion = table(cv$label, cv$prediction)
  acc = (confusion[1,1] + confusion[2,2]) / nrow(cv)
  cv.accuracy = c(cv.accuracy, acc)
}

cv.check = tibble(
  alpha = alpha.grid,
  accuracy = cv.accuracy
)
cv.check
```

As we increase alpha, the accuracy seems to decrease. Let's stick with 0.1 for our alpha value.

```{r}
best.alpha = 0.1
for (a in alpha.grid) {
  spam.probs = list()
  ham.probs = list()
  
  for (v in vocabulary) {
    
    spam.probs[[v]] = (spam.counts[[v]] + best.alpha) / (n.spam + best.alpha * n.vocabulary)
    ham.probs[[v]] = (ham.counts[[v]] + best.alpha) / (n.ham + best.alpha * n.vocabulary)
  }
}

spam.test = spam.test %>%
  mutate(prediction = unlist(map(sms, classify))) %>%
  select(label, prediction, sms)

confusion = table(spam.test$label, spam.test$prediction)
test.accuracy = (confusion[1,1] + confusion[2,2]) / nrow(spam.test)
test.accuracy
```

Using our best alpha value of 0.1, we were able to achieve about 93% accuracy on our test set. This is a fairly good result.